% @Author: athul
% @Date:   2014-09-15 12:11:31
% @Last Modified by:   Athul Vijayan
% @Last Modified time: 2014-09-25 23:11:26

\documentclass[11pt,paper=a4,answers]{exam}
\usepackage{graphicx,lastpage}
\usepackage{subfig}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{upgreek}
\usepackage{float}
\usepackage{placeins}
\usepackage[bookmarks]{hyperref}
\usepackage{censor}
\usepackage{amsmath}
\usepackage{amssymb, amsthm}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\usepackage{bm}
\usepackage{caption}
\usepackage{enumerate}

\newcommand{\cb}[1]{{\cellcolor{black! 15 }$ #1$}}
\newcommand{\cw}[1]{{\cellcolor{black! 35 }$ \color{white} #1$}}
\censorruledepth=-.2ex
\censorruleheight=.1ex
\hyphenpenalty 10000
\usepackage[paperheight=10.5in,paperwidth=8.27in,bindingoffset=0in,left=0.8in,right=1in,
top=0.7in,bottom=1in,headsep=.5\baselineskip]{geometry}
\flushbottom
\usepackage[normalem]{ulem}
\renewcommand\ULthickness{2pt}   %%---> For changing thickness of underline
\setlength\ULdepth{1.5ex}%\maxdimen ---> For changing depth of underline
\renewcommand{\baselinestretch}{1}
\pagestyle{empty}
\renewcommand{\vec}[1]{\mathbf{#1}}
\pagestyle{headandfoot}
\headrule

\newcommand{\continuedmessage}{%
\ifcontinuation{\footnotesize continues\ldots}{}%
 }
\runningheader{\footnotesize \today}
{\footnotesize Pattern Recognition}
{\footnotesize Page \thepage\ of \numpages}
\footrule
\footer{\footnotesize}
{}
{\ifincomplete{\footnotesize section \IncompleteQuestion\ continues
on the next page\ldots}{\iflastpage{\footnotesize End}{\footnotesize Please go        on to the next page\ldots}}}

\usepackage{cleveref}
\crefname{figure}{figure}{figures}
\crefname{question}{question}{questions}
%==============================================================
\begin{document}

\noindent
\begin{minipage}[l]{.1\textwidth}%
\noindent
\end{minipage}
\hfill
\begin{minipage}[r]{.68\textwidth}%
\begin{center}
{\large \bfseries \par
\Large Pattern Recognition Assignment 2 \\[2pt]
\vspace{6pt}
\small   \par}
\end{center}
\end{minipage}
\begin{minipage}[l]{.195\textwidth}%
\noindent
{\footnotesize}
\end{minipage}
\par
\noindent
\uline{Group 12 \hfill \normalsize\emph \hfill       Athul Vijayan (ED11B004) \& KIRAN KUMAR.G.R (AM14D405)}\\
\begin{questions}
% ============================== Content starts here
\question \textbf{Theory}\\
    \begin{enumerate}[]
        \item $\vec{x} = [x_1, x_2, \cdots, x_d] \in \mathbb{R}^d$ is the \textit{feature vector} in a vector space called \textit{feature space} containing $d$ continuous features in it.
        \item $\boldsymbol\omega = [\omega_1, \omega_2, \cdots, \omega_c]$ be the c finite \textit{state of nature} / catogories.
    \end{enumerate}
    We follow from bayes theorem that
        $$ P(\omega_i|\vec{x}) = \frac{P(\vec{x}|\omega_i) P(\omega_i)}{P(\vec{x})}$$
    In bayesian classification we decide to take action $\alpha_i$ for which Conditional risk $R(\alpha_i | \vec{x})$ is minimum. That is, maximum discriminant will correspond to mimimum conditional risk. And for minimum error rate classifier, we can define $g_i(\vec{x}) = P(\omega_i|\vec{x})$. Maximum discriminant function corresponds to the maximum posterior probability. Also if $f(.)$ is a monotonously increasing function, then $f(g_i(\vec{x}))$ will also give same classifier.
    \begin{align*}
        g_i(x) &= P(\omega_i | \vec{x}) \\
        &= P(\vec{x}|\omega_i) P(\omega_i) \\
        &= ln(P(\vec{x}|\omega_i)) + ln(P(\omega_i)) \qquad |\quad ln(.) \text{ is monotonously increasing}
    \end{align*}
    For a dataset with $c$ classes denoted as $\omega_1, \omega_2, \cdots, \omega_c$, we assume the likelihood probability in each class $\omega_i$ is distributed as Multivariate Gaussian. i.e. $P(\vec{x} | \omega_i) \sim \mathcal{N}(\bm{\mu}_i, \vec{\Sigma}_i)$.\\
        Our General Bayes classifier becomes:
        $$g_i(\vec{x}) = -{1 \over 2} (\vec{x} - \bm{\mu}_i) \bm{\Sigma}_i ^{-1} (\vec{x} - \bm{\mu}_i)^T - {d \over 2}\ln2\pi  - {1 \over 2}\ln|\bm{\Sigma}_i| + \ln P(\omega_i)$$
        Now depending on the covariance matrix $\bm{\Sigma}_i$, various cases can be generated.\\
        One Interesting case is that when $\bm{\Sigma}_i$ is diagonal, the covariance/ correlation between any two features is zero. That implies that we do not get any information about feature $x_i$ from features $x_j$ if $j \neq i$. In terms of probability, $P(x_i|\omega_k, \{x_j \quad \forall \quad j \neq i\}) = P(x_i| \omega_k)$. This is called \textbf{naive bayes} classifier.
    \begin{enumerate}[i.]
        \item \textbf{Bayesian Classifier with Covariance same for all classes.} $\bm{\Sigma_i} = \bm{\Sigma}$.\\
        
        Since $\bm{\Sigma_i} = \bm{\Sigma}$, we can drop second and third terms because they are independent of $i$ and will be same for every class. making it
        \begin{align*}
            g_i(\vec{x}) &= -{1 \over 2} (\vec{x} - \bm{\mu}_i) \bm{\Sigma}_i ^{-1} (\vec{x} - \bm{\mu}_i)^T + \ln P(\omega_i)\\
            &= \left(\bm{\Sigma^{-1}} \bm{\mu}_i\right)^T \vec{x} - {1 \over 2}\bm{\mu}_i ^T \bm{\Sigma} ^{-1}\bm{\mu_i} + \ln P(\omega_i)
        \end{align*}
        We can see this is a linear classifier. If we assume every class conditional probability is distributed as Multivariate Gaussian with same Covariance $\bm{\Sigma}$ like this case, the resulting classifier is called linear discriminant.\\
        Now we need to get estimated $\bm{\mu}_i$ and $\bm{\Sigma}$. Clearly if we calculate the covariance of measured data in each class, they will not be similar across classes. So we find estimate for parameters by maximizing likelihood.
        \begin{enumerate}[]
            \item let sampes in class $\omega_i$ be denoted as $\mathcal{D}_i$ such that the total samples can be expressed as $\mathcal{D} = \{\mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_c\}$. Let number of samples in $\mathcal{D}_i$ be $N_i$ and number of total samples, i. e number of points in $\mathcal{D}$ is $N$.
            \item In the generative learning approach, we assume each of $\mathcal{D}_i$ is distributed as gaussian with mean $\bm{\mu}_i$ and covariance $\bm{\Sigma}_i$. Note that in LDA, covariance of every $\mathcal{D}_i$ is same.
        \end{enumerate}
        Log Likelihood function is given by:
        $$l(\bm{\theta}) = \ln P(\mathcal{D}| \bm{\theta}))$$
        and $\bm{\theta}$ that maximizes $l(\bm{\theta})$ is 
        $$\bm{\hat{\theta}} = \argmax_{\bm{\theta}} \quad l(\bm{\theta})$$
        With gaussian aproximation, 
        \begin{align*}
            P(\mathcal{D}| \bm{\theta}) &= \prod _{i=1}^N P(\vec{x}_i| \omega)\\
            &= \prod _{k=1}^c \prod _{\vec{x}_i \in \mathcal{D}_k} P(\vec{x}_i | \omega_k) \\ 
            \Rightarrow l(\bm{\theta}) &= \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} \ln P(\vec{x}_i | \omega_k) \\
            &= \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2}(\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T - {1 \over 2}\ln |\bm{\Sigma}|\\
        \end{align*}
        Now to estimate parameters, we maximize $l(\bm{\theta})$. 
        \begin{align*}
            \nabla _{\bm{\theta}} l(\bm{\theta}) = \bm{0}
        \end{align*}
        Now to find $\bm{\mu}_j$ of gaussian pdf of class $P(\vec{x}| \omega_j)$,
        \begin{align*}
            \nabla _{\bm{\mu}_j} l(\bm{\theta}) &= 0 \\
            \nabla _{\bm{\mu}_j} \left( \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2}(\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T - {1 \over 2} \ln |\bm{\Sigma}| \right) &= 0
        \end{align*}
        For $k \neq j$, we can see all the terms goes to zero. we can denote this estimated mean for class $j$ as $\hat{\bm{\mu}}_j$.
        \begin{align}
            \boxed{\hat{\bm{\mu}}_j = {1 \over N_j} \sum_{\vec{x}_i \in \mathcal{D}_j} \vec{x}_i }
        \end{align}
        And for $\bm{\Sigma}$, 
        \begin{align*}
            \nabla _{\bm{\Sigma}} l(\bm{\theta}) &= 0 \\
            \nabla _{\bm{\Sigma}} \left( \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2}(\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T - {1 \over 2} \ln |\bm{\Sigma}| \right) &= 0\\
            \nabla _{\bm{\Sigma}} \left( \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2} tr\left((\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T\right) - {1 \over 2} \ln |\bm{\Sigma}| \right) &= 0\\
            \nabla _{\bm{\Sigma}} \left( \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2} tr\left(\bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T (\vec{x}_i - \bm{\mu}_k)\right) - {1 \over 2} \ln |\bm{\Sigma}| \right) &= 0 \\
            \nabla _{\bm{\Sigma}} \left( \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2} tr\left(\bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T (\vec{x}_i - \bm{\mu}_k)\right)  \right) - {N \over 2} \bm{\Sigma}^{-1} &= 0      
        \end{align*}
        Above, we have used properties,
        \begin{itemize}
             \item $tr(\text{Real number}) =$ Real Number
             \item $tr(ABC) = tr(BCA)$
             \item $\nabla _A |A| = A^{-1}$
         \end{itemize}
        That gives us,
        \begin{align}
        \boxed{\hat{\bm{\Sigma}} = {1 \over N}\sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} (\vec{x}_i - \bm{\mu}_k)^T (\vec{x}_i - \bm{\mu}_k)}
        \end{align}
        is the best estimate of $\bm{\Sigma}$ in the maximum likelihood sense. The parameter is biased.

        \item \textbf{Bayesian Classifier with Covariance different for all classes}
        It is straight forward, we can neglect only the constant term in the general discriminant function.
        $$g_i(\vec{x}) = -{1 \over 2} (\vec{x} - \bm{\mu}_i) \bm{\Sigma}_i ^{-1} (\vec{x} - \bm{\mu}_i)^T - {1 \over 2}\ln|\bm{\Sigma}_i| + \ln P(\omega_i)$$
        This is a Quadratic Discriminant function. We assume each for class $k$ conditional probabilities belong to a gaussian distribution with $\bm{\mu}_k$ and $\bm{\Sigma}_k$.\\
        To estimate this parameters using maximum likelihood estimation.
        \begin{align*}
            P(\mathcal{D}_k| \bm{\theta}_k) &= \prod _{i=1}^{N_k} P(\vec{x}_i| \omega_k)\\
            \Rightarrow l(\bm{\theta}) &= \sum_{\vec{x}_i \in \mathcal{D}_k} \ln P(\vec{x}_i | \omega_k) \\
            &= \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2}(\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}_k^{-1} (\vec{x}_i - \bm{\mu}_k)^T - \ln |\bm{\Sigma}_k|\\
        \end{align*}
        Now to estimate parameters, we maximize $l(\bm{\theta})$. 
        \begin{align*}
            \nabla _{\bm{\theta}} l(\bm{\theta}) = \bm{0}
        \end{align*}
        Now to find $\bm{\mu}_k$ of gaussian pdf of class $P(\vec{x}| \omega_k)$,
        \begin{align*}
            \nabla _{\bm{\mu}_k} l(\bm{\theta}) &= 0 \\
            \nabla _{\bm{\mu}_k} \left( \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2}(\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T - {1 \over 2} \ln |\bm{\Sigma}| \right) &= 0\\
        \end{align*}
        That gives the sample mean of the class as the optimal parameter.
        $$\boxed{\hat{\bm{\mu}}_k = {1 \over N_k} \sum_{\vec{x}_i \in \mathcal{D}_k} \vec{x}_i }$$
        Now for Covariance estimation,
        \begin{align*}
            \nabla _{\bm{\Sigma}_k} l(\bm{\theta}) &= 0 \\
            \nabla _{\bm{\Sigma}_k} \left( \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2}(\vec{x}_i - \bm{\mu}_k) \bm{\Sigma}^{-1} (\vec{x}_i - \bm{\mu}_k)^T - {1 \over 2} \ln |\bm{\Sigma}| \right) &= 0\\
        \end{align*}
        Using properties used above,  we find optimal estimate is the biased covariance of class $k$.
        \begin{align}
            \boxed{\hat{\bm{\Sigma}}_k = {1 \over N_k} \sum_{\vec{x}_i \in \mathcal{D}_k} (\vec{x}_i - \bm{\mu}_k)^T (\vec{x}_i - \bm{\mu}_k)}
        \end{align}
        \item \textbf{Naive Bayes}\\
            With naive bayes, we have $\bm{\Sigma}_{ij} = 0 \qquad \forall \quad i \neq j$ diagonal. Intiuitively, it means that we do not get any information about a feature if we know the value of any other feature vector. We can write our joint class conditional probability as.
            \begin{align*}
                P(\vec{x}| \omega_k) &= P(x_1, x_2, \cdots, x_d | \omega_k) \qquad \text{$x_i$ are individual features}\\
                &= P(x_1| \omega_k) P(x_2, x_3, \cdots, x_d| \omega_k, x_1)\\
                &= P(x_1| \omega_k) P(x_2| \omega_k, x_1) P(x_3, \cdots, x_d| \omega_k, x_1, x_2)\\
                \vdots\\
                &= P(x_1| \omega_k) P(x_2| \omega_k, x_1)  \cdots P(x_d| \omega_k, x_1, x_2, \cdots, x_{d-1})\\
            \end{align*}
            Now naive payes assumes that, features are independent that is,
            $$P(x_i|\omega_k, \{x_j \quad \forall \quad j \neq i\}) = P(x_i| \omega_k)$$
            That gives us, 
            \begin{align*}
            P(\vec{x} \vert \omega_k) &= P(x_1 \vert \omega_k) P(x_2 \vert \omega_k) \cdots P(x_d \vert \omega_k)\\    
            &= \prod _{i=1}^d P(x_i \vert \omega_k)
            \end{align*}
            Now we assumed already that $P(\vec{x}| \omega_k) \sim \mathcal{N}(\bm{\mu}_k , \bm{\Sigma}_k)$. Now with independence condition, we can say $P(x_i| \omega_k) \sim \mathcal{N}(\mu_i , \sigma_i^2)$ where $\mu_i = \bm{\mu}_i$ and $\sigma_i^2 = \bm{\Sigma}_{ii}$. This means each feature itself is distributed as univariate gaussian with mean and variance corresponding to that feature alone. We will find the optimum estimate of these parameters now:\\
            \begin{itemize}
                \item \textbf{Naive Bayes with $\bm{C} = \sigma ^2 \bm{\mathrm{I}}$}\\
                Like in bayesian case, we assume a gaussian distribution with different covariance for every class.\\
                Let covariance of class $\omega_k$ be in the form of $\bm{\Sigma}_k = \sigma_k ^2 \bm{I}$. Here again, we find the optimum estimation for scalar $\sigma_k ^2$ using maximizing likelihood function.\\
                Let $C = \sigma_k^2$
                \begin{align*}
                    l(\bm{\theta}) &= \sum_{\vec{x}_i \in \mathcal{D}_k} \ln P(\vec{x}_i| \omega_k) \\
                    &= \sum_{\vec{x}_i \in \mathcal{D}_k} -{1 \over 2} (\vec{x}_i - \bm{\mu}_k){1 \over C}\bm{I} (\vec{x}_i - \bm{\mu}_k)^T -{1 \over 2} \ln C
                \end{align*}
                Now differentiating and equating to zero, we get estimate of $\bm{\mu}$ as
                $$\boxed{\hat{\bm{\mu}}_k = {1 \over N_k} \sum_{\vec{x}_i \in \mathcal{D}_k} \vec{x}_i }$$

                and optimal value of $\sigma_k ^2$ is:
                \begin{align}
                    \boxed{\hat{\sigma}_k ^2 = {1 \over N_k} \sum_{\vec{x}_i \in \mathcal{D}_k} (\vec{x}_i - \bm{\mu}_k) (\vec{x}_i - \bm{\mu}_k)^T}
                \end{align}
                Note that it is a scalar for every class.


                \item \textbf{Naive Bayes with Different covariance for all classes}\\
                Like in bayesian case, we assume a gaussian distribution with different covariance for every class.\\
                Now to estimate optimum parameters for the model, we use likelihood maximizing as before.
                \begin{align*}
                    l(\bm{\theta}_k) &= \sum_{\vec{x}_i \in \mathcal{D}_k} \sum_{i=1}^d \ln P(x_i| \omega_k)\\
                \end{align*}
                
                Now, $\frac{\partial l(\bm{\theta}_k)}{\partial \bm{\mu}_k} = 0$ gives us,
                $$\boxed{\hat{\bm{\mu}}_k = {1 \over N_k} \sum_{\vec{x}_i \in \mathcal{D}_k} \vec{x}_i }$$
                Now, $\frac{\partial l(\bm{\theta}_k)}{\partial \sigma_d} = 0$ will give us the optimum variance $\sigma _d^2$ of the univariate gaussian distribution where feature $x_d$ belongs.
                $$ \sigma _d^2 = {1 \over N_k}\sum_{\vec{x}_i \in \mathcal{D}_k} ((x_d)_i - \mu_d)^2$$
                So best $\bm{\Sigma}$ is:
                $$\boxed{(\hat{\bm{\Sigma}}_k)_{lm} =
                \begin{cases}
                    {1 \over N_k}\sum_{\vec{x}_i \in \mathcal{D}_k} ((x_l)_i - \mu_l)^2 &\text{ if } l = m\\
                    0 & \text{otherwise}
                \end{cases} }$$
                So the best estimate has variance of each feature along its diagonals and zero everywhere else.

                \item \textbf{Naive Bayes with Same covariance for all classes}\\
                Like in bayesian case, we maximize the overall likelihood function.\\
                \begin{align*}
                    P(\mathcal{D}| \bm{\theta}) &= \prod _{i=1}^N P(\vec{x}_i| \omega)\\
                    &= \prod _{k=1}^c \prod _{\vec{x}_i \in \mathcal{D}_k} \prod _{m=1}^d P((x_m)_i | \omega_k) \\ 
                    \Rightarrow l(\bm{\theta}) &= \sum_{k=1}^c \sum_{\vec{x}_i \in \mathcal{D}_k} \sum_{m=1}^d \ln P((x_m)_i | \omega_k) \\
                \end{align*}
                Maximizing this by differentiating gives us:
                $$\boxed{\hat{\bm{\mu}}_k = {1 \over N_k} \sum_{\vec{x}_i \in \mathcal{D}_k} \vec{x}_i }$$
                and optimum $\bm{\Sigma}$ is:
                $$\boxed{ (\hat{\bm{\Sigma}})_{lm} =
                \begin{cases}
                    {1 \over N}\sum_{k=1}^c\sum_{\vec{x}_i \in \mathcal{D}_k} ((x_l)_i - \mu_l)^2 &\text{ if } l = m\\
                    0 & \text{otherwise}
                \end{cases} }$$
                So the best estimate has cumulative variance of each feature along its diagonals and zero everywhere else.
            \end{itemize}
            \item \textbf{Performance evaluation methods}
            \begin{enumerate}
                \item \textbf{Confusion matrix and Performance metrics}\\
                    Confusion matrix or an error matrix is a specific table layout that allows visualization of the performance of an algorithm. The structure of the matrix is as follows, 
                    \begin{itemize}
                        \item Rows correspond to classes in the test set.
                        \item Columns correspond to classes in the classification result.
                        \item The diagonal elements in the matrix represent the number of correctly classified data points of each class, i.e. the number of data points with a certain class name that actually obtained the same class name during classification.
                        \item The off-diagonal elements represent misclassified data points or the classification errors, i.e. the number of data points that ended up in another class during classification. 
                        \item Off-diagonal row elements represent data points of a certain class which were excluded from the respective class during classification. Such errors are known as errors of omission or exclusion. It is the ratio sum of off diagonal row elements to the sum total off the row.
                        \item Off-diagonal column elements represent data points of other classes that were included in a certain classification class. Such errors are known as errors of commission or inclusion. It is the ratio sum of off diagonal column elements to the sum total off the column.
                    \end{itemize}
                    
                    \textbf{Accuracy} is the fraction of correctly classified data points with respect to all data points of the given class. It is ratio of sum of the diagonal elements to the grand total of the matrix elements.\\
                    \textbf{Precision} is the proportion of the predicted positive cases that were correct. It is the ratio of the corresponding diagonal element of the class to the sum the off-diagonal column elements.
                \item \textbf{Receiver Operating Characteristics (ROC)}\\
                    A receiver operating characteristics (ROC) curve is a technique used for visualizing performance of classifiers. ROC graphs are two-dimensional graphs in which True Positive rate is plotted along the Y axis and False Positive rate is plotted on the X axis. An ROC graph depicts relative trade-offs between true positives and false positives. Interpretations from ROC curves are,
                    \begin{itemize}
                        \item It shows the tradeoff between sensitivity (True positive rate) and class specificity (any increase in sensitivity will be accompanied by a decrease in specificity).
                        \item The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.
                        \item The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the classification.
                    \end{itemize}
                \item \textbf{Detection error tradeoff (DET)}
                    In the DET curve, plot consists of error rates on both axes, giving uniform treatment to both types of error, and use a scale for both axes which spreads out the plot. It better distinguishes different well performing systems. The DET curves are approximately straight lines, corresponding to normal likelihood distributions. The curves are limited to the lower left quadrant if the performance is good. The closer the curve comes to the x=-y diagonal of the DET plot, more random is the performance.
            \end{enumerate}
                
    \end{enumerate}
\newpage
\question \textbf{Implementation And results}
\begin{enumerate}[i.]
    \item \textbf{The linearly sepearble data with three classes}.\\
        First step is to visualize the data and predict the accuracy of our algorithms intuitively. This will help us identify outliers and bad samples in data and plan ahead accordingly. Figure ~\ref{fig:scatterData1} shows scatter plot.\\
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{data1}
            \vspace{-30pt}
            \caption{Scatter plot of Linearly sepearble data}
            \label{fig:scatterData1}
        \end{figure}\\
        As it is evident from visual inspection, the data looks clean in the sense that it does not have much outliers or missing feature etc.. So we will apply our algorithms to this data with different choices of covariance. As the each classes are well seperated, we can expect a good accuracy for our model.
        \begin{enumerate}
            \item \textbf{Case 1: Bayes with Covariance same for all classes}\\
            We have used the estimates for optimum mean and covariance as we derived earlier. We constructed the gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure ~\ref{fig:data1g1} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1gaussian1}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of linearly sepearble data with bayesian model with same covariance}
                \label{fig:data1g1}
            \end{figure}\\
            We can visualize the decision boundary better by examining the contour plot and the Discriminant boundary. The test data is plotted, the contours of each class conditional probabilities are also plotted. The plot is Figure ~\ref{fig:data1c1}.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1c1}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for linear sepearble data with bayesian model with same covariance}
                \label{fig:data1c1}
            \end{figure}\\
            As we can see, The shape of all the gaussians are similar. This is caused by the same covariance of each classes. As the gaussians are same in shape, only their mean difffer. So it gives us a linear discriminant function of which boundaries are shown.\\
            The decision boundary passes through the midpoint of line segment joining the means of any pair of classes. This is because of equal prior probability.
            
            \item \textbf{Case 2: Bayes with Covariance different for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data1g2} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1gaussian2}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of linearly seperable data for bayesian model with different covariance}
                \label{fig:data1g2}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot.The plot is Figure~\ref{fig:data1c2}.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1c2}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for linear sepearble data for bayesian model with different covariance}
                \label{fig:data1c2}
            \end{figure}\\
            As we can see, The shape of all the gaussians are dissimilar as the covariance is different for each class. So it gives us a non linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.

            \item \textbf{Case 3: Naive Bayes with} $\bm{\Sigma}_k = \sigma_k^2\bm{I}$\\
            We expect a symmetric gaussian. Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data1g3} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1gaussian3}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of linearly seperable data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:data1g3}
            \end{figure}\\
            We expect concentric circles in the contour plot. The plot is Figure~\ref{fig:data1c3}.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1c3}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for linear seperble data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:data1c3}
            \end{figure}
            As we can see, The shape of all the gaussians are symmetrical / circular contours.The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.\\

            \item \textbf{Case 4: Naive Bayes with same covariance for all classes}\\
            As the covariance is a diagonal matrix with arbitrary elements, we expect a gaussian whose contours are elliptical. Again, since covariance of each class is same, we expect a linear boundary. Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data1g4} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1gaussian4}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of linearly seperable data for Naive Bayes with same covariance}
                \label{fig:data1g4}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:data1c4}.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1c4}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for linear seperble data for Naive Bayes with different covariance}
                \label{fig:data1c4}
            \end{figure}
            The covariance is same for each class, So it gives us a linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.

            \item \textbf{Case 5: Naive Bayes with different covariance for all classes}\\
            Since covariance of each class is different, we expect a non linear boundary. Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data1g5} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1gaussian5}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of linearly seperable data for Naive Bayes with different covariance}
                \label{fig:data1g5}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. The plot is Figure~\ref{fig:data1c5}.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data1c5}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for linear seperble data for Naive Bayes with different covariance}
                \label{fig:data1c5}
            \end{figure}
            The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.
            \item \textbf{Performance evaluation}
                The confusion matrix and all the performance metrics are same for every algorithm we used. It is given as:
                \begin{table}[ht]
                    \centering
                    \minipage{0.75\textwidth}
                        \begin{tabular}{c | c c c c | c | c |}
                            \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                             & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                            \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                            & Class 1       & \cw{150}  & \cb{0}    & \cb{0}    & \cb{150}  &\cb{0}\\ 
                            & Class 2       & \cb{0}    & \cw{150}  & \cb{0}    & \cb{150}  &\cb{0}\\ 
                            & Class 3       & \cb{0}    & \cb{0}    & \cw{150}  & \cb{150}  &\cb{0}\\ 
                            \cline{2-7}
                            & Total         & \cb{150}  & \cb{150}  & \cb{150}  & \cb{450}  &\cb{0}\\ 
                            \cline{2-7}
                            & Excl. Error   & \cb{0}    & \cb{0}    & \cb{0}    & \cb{0}    &\cb{}\\ 
                            \cline{2-7}
                        \end{tabular}
                        \caption{Confusion matrix for Linearly seperable data, All Algorithm}
                    \endminipage\hfill
                    \minipage{0.25\textwidth}
                        \begin{tabular}{| l | c | c |}
                            \hline
                            & Precision & Accuracy\\
                            \hline
                            Class 1 & 1.00 & \\
                            \cline{1-2}
                            Class 2 & 1.00 & 100 \%\\
                            \cline{1-2}
                            Class 3 & 1.00 & \\
                            \hline
                        \end{tabular}
                        \caption{Performance metric}
                    \endminipage\hfill
                    \label{tab:confmat}
                \end{table}\\
                And ROC curve is not necessary in this case since all algorithms give same full accuracy.
        \end{enumerate}

    \FloatBarrier
    \item \textbf{Non linearly seperable data with three classes}.\\
        Like before, first we visualize data and draw some conclusions. This will help us identify outliers and bad samples in data and plan ahead accordingly. Figure ~\ref{fig:scatterData2} shows scatter plot.\\
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{data2}
            \vspace{-30pt}
            \caption{Scatter plot of Linearly sepearble data}
            \label{fig:scatterData2}
        \end{figure}\\
        Since there are less overlapping data, we can tell that accuracy will be good generally.
        \begin{enumerate}
            \item \textbf{Case 1: Bayes with Covariance same for all classes}\\
            We have used the estimates for optimum mean and covariance as we derived earlier. We constructed the gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure ~\ref{fig:data2g1} shows the plot.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2gaussian1}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of non linearly sepearble data with bayesian model with same covariance}
                \label{fig:data2g1}
            \end{figure}\\
            We can visualize the decision boundary better by examining the contour plot and the linear Discriminant boundary. The test data is plotted, the contours of each class conditional probabilities are also plotted. The plot is Figure ~\ref{fig:data2c1}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2c1}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non linear sepearble data with bayesian model with same covariance}
                \label{fig:data2c1}
            \end{figure}\\
            It gives us a linear discriminant function of which boundaries are shown.\\
                        
            \item \textbf{Case 2: Bayes with Covariance different for all classes}\\
            gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data2g2} shows the plot.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2gaussian2}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of non linearly seperable data for bayesian model with different covariance}
                \label{fig:data2g2}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot.The plot is Figure~\ref{fig:data2c2}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2c2}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non linear sepearble data for bayesian model with different covariance}
                \label{fig:data2c2}
            \end{figure}\\
            It gives us a non linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.

            \item \textbf{Case 3: Naive Bayes with} $\bm{\Sigma}_k = \sigma_k^2\bm{I}$\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data2g3} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2gaussian3}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of non linearly seperable data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:data2g3}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric circles in the contour plot. The plot is Figure~\ref{fig:data2c3}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2c3}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non linear seperble data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:data2c3}
            \end{figure}
            As we can see, The shape of all the gaussians are symmetrical / circular contours.The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.\\

            \item \textbf{Case 4: Naive Bayes with same covariance for all classes}\\
            Again, since covariance of each class is same, we expect a linear boundary. Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data2g4} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2gaussian4}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of non linearly seperable data for Naive Bayes with same covariance}
                \label{fig:data2g4}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:data2c4}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2c4}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non linear seperble data for Naive Bayes with different covariance}
                \label{fig:data2c4}
            \end{figure}
            The covariance is same for each class, So it gives us a linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.

            \item \textbf{Case 5: Naive Bayes with different covariance for all classes}\\
            we expect a non linear boundary. Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data2g5} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2gaussian5}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of non linearly seperable data for Naive Bayes with different covariance}
                \label{fig:data2g5}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:data2c5}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data2c5}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non linear seperble data for Naive Bayes with different covariance}
                \label{fig:data2c5}
            \end{figure}
            The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown. As the data set is well seperated, the predictor gives a good accuracy.
            \item \textbf{Performance evaluation}
                The confusion matrix and all the performance metrics are same for every algorithm we used. It is given in Table ~\ref{tab:d2con}:
                \begin{table}[ht]
                    \centering
                    \minipage{0.75\textwidth}
                        \begin{tabular}{c | c c c c | c | c |}
                            \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                             & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                            \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                            & Class 1       & \cw{150}  & \cb{0}    & \cb{0}    & \cb{150}  &\cb{0}\\ 
                            & Class 2       & \cb{0}    & \cw{150}  & \cb{0}    & \cb{150}  &\cb{0}\\ 
                            & Class 3       & \cb{0}    & \cb{0}    & \cw{150}  & \cb{150}  &\cb{0}\\ 
                            \cline{2-7}
                            & Total         & \cb{150}  & \cb{150}  & \cb{150}  & \cb{450}  &\cb{0}\\ 
                            \cline{2-7}
                            & Excl. Error   & \cb{0}    & \cb{0}    & \cb{0}    & \cb{0}    &\cb{}\\ 
                            \cline{2-7}

                        \end{tabular}
                        \caption{Confusion matrix for Non Linearly seperable data, All Algorithm}
                        \label{tab:d2con}
                    \endminipage\hfill
                    \minipage{0.25\textwidth}
                        \begin{tabular}{| l | c | c |}
                            \hline
                            & Precision & Accuracy\\
                            \hline
                            Class 1 & 1.00 & \\
                            \cline{1-2}
                            Class 2 & 1.00 & 100 \%\\
                            \cline{1-2}
                            Class 3 & 1.00 & \\
                            \hline
                        \end{tabular}
                        \caption{Performance metric}
                    \endminipage\hfill
                    
                \end{table}\\
                And ROC curve is not necessary in this case since all algorithms give same full accuracy.
        \end{enumerate}
    \FloatBarrier

    \item \textbf{Overlapping Data with three classes}\\
        As always, first step is to visualize the data.Lets scatterplot the data with different markers for each class. The scatterplot is given in Figure~\ref{fig:scatterData3}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{data3}
            \vspace{-30pt}
            \caption{Scatter plot of Overlapping data}
            \label{fig:scatterData3}
        \end{figure}\\
        As we can see there is no clear boundary seperating each class, the accuracy will be less than than previous cases. But it is evident that majority of data belonging to each class is at the centre. The number of data points which are in the boundary are less. Even though data is overlapping, there aren't much outliers/ missing features. So we skip the preprocessing of data.\\
        Now lets make classification models for the data and analyse the results.
        \begin{enumerate}
            \item \textbf{Case 1: Bayes with Covariance same for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure ~\ref{fig:data3g1} shows the plot.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3gaussian1}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of overlapping data with bayesian model with same covariance}
                \label{fig:data3g1}
            \end{figure}\\
            Note the gaussians are not well seperated like previous case, which is expected of overlapping data.We can visualize the decision boundary better by examining the contour plot and the linear Discriminant boundary. The test data is plotted, the contours of each class conditional probabilities are also plotted. The plot is Figure ~\ref{fig:data3c1}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3c1}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non Overlapping data with bayesian model with same covariance}
                \label{fig:data3c1}
            \end{figure}\\
            As the gaussians are same in shape, only their mean difffer. So it gives us a linear discriminant function of which boundaries are shown. The contour lines are crossing each other because of overlapping data.\\
            The performance matrix is given in Table~\ref{tab:d3con1}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{144}  & \cb{3}    & \cb{3}    & \cb{150}  &\cb{0.04}\\ 
                        & Class 2       & \cb{1}    & \cw{149}  & \cb{0}    & \cb{150}  &\cb{0.0066}\\ 
                        & Class 3       & \cb{1}    & \cb{0}    & \cw{149}  & \cb{150}  &\cb{0.006}\\ 
                        \cline{2-7}
                        & Total         & \cb{146}  & \cb{152}  & \cb{152}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.013}    & \cb{0.019}    & \cb{0.019}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Overlapping data, Case 1 Algorithm}
                    \label{tab:d3con1}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.96 & \\
                        \cline{1-2}
                        Class 2 & 0.993 & 98.22 \%\\
                        \cline{1-2}
                        Class 3 & 0.993 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\
                        
            \item \textbf{Case 2: Bayes with Covariance different for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data3g2} shows the plot.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3gaussian2}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of overlapping data for bayesian model with different covariance}
                \label{fig:data3g2}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot.The plot is Figure~\ref{fig:data3c2}.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3c2}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for overlapping data for bayesian model with different covariance}
                \label{fig:data3c2}
            \end{figure}\\
            As the data set is overlapping, the classification accuracy is expected to be less than previous cases.
            The performance matrix is given in Table~\ref{tab:d3con2}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{147}  & \cb{1}    & \cb{2}    & \cb{150}  &\cb{0.02}\\ 
                        & Class 2       & \cb{2}    & \cw{148}  & \cb{0}    & \cb{150}  &\cb{0.0133}\\ 
                        & Class 3       & \cb{1}    & \cb{0}    & \cw{149}  & \cb{150}  &\cb{0.006}\\ 
                        \cline{2-7}
                        & Total         & \cb{150}  & \cb{149}  & \cb{151}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.02}    & \cb{0.0067}    & \cb{0.013}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Overlapping data, Case 2 Algorithm}
                    \label{tab:d3con2}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.98 & \\
                        \cline{1-2}
                        Class 2 & 0.986 & 98.66 \%\\
                        \cline{1-2}
                        Class 3 & 0.993 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\

            \item \textbf{Case 3: Naive Bayes with} $\bm{\Sigma}_k = \sigma_k^2\bm{I}$\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data3g3} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3gaussian3}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of overlapping data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:data3g3}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric circles in the contour plot. The plot is Figure~\ref{fig:data3c3}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3c3}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for overlapping data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:data3c3}
            \end{figure}
            As we can see, The shape of all the gaussians are symmetrical / circular contours.The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown.\\
            The performance matrix is given in Table~\ref{tab:d3con3}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{146}  & \cb{2}    & \cb{2}    & \cb{150}  &\cb{0.02}\\ 
                        & Class 2       & \cb{2}    & \cw{148}  & \cb{0}    & \cb{150}  &\cb{0.0133}\\ 
                        & Class 3       & \cb{1}    & \cb{0}    & \cw{149}  & \cb{150}  &\cb{0.006}\\ 
                        \cline{2-7}
                        & Total         & \cb{149}  & \cb{150}  & \cb{151}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.02}    & \cb{0.013}    & \cb{0.013}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Overlapping data, Case 3 Algorithm}
                    \label{tab:d3con3}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.973 & \\
                        \cline{1-2}
                        Class 2 & 0.986 & 98.44 \%\\
                        \cline{1-2}
                        Class 3 & 0.993 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\

            \item \textbf{Case 4: Naive Bayes with same covariance for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data3g4} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3gaussian4}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of overlapping data for Naive Bayes with same covariance}
                \label{fig:data3g4}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:data3c4}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3c4}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for overlapping data for Naive Bayes with different covariance}
                \label{fig:data3c4}
            \end{figure}
            The covariance is same for each class, So it gives us a linear discriminant function of which boundaries are shown.
            The performance matrix is given in Table~\ref{tab:d3con4}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{145}  & \cb{3}    & \cb{2}    & \cb{150}  &\cb{0.033}\\ 
                        & Class 2       & \cb{1}    & \cw{149}  & \cb{0}    & \cb{150}  &\cb{0.066}\\ 
                        & Class 3       & \cb{1}    & \cb{0}    & \cw{149}  & \cb{150}  &\cb{0.006}\\ 
                        \cline{2-7}
                        & Total         & \cb{147}  & \cb{152}  & \cb{151}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.013}    & \cb{0.019}    & \cb{0.013}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Overlapping data, Case 4 Algorithm}
                    \label{tab:d3con4}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.966 & \\
                        \cline{1-2}
                        Class 2 & 0.993 & 98.44 \%\\
                        \cline{1-2}
                        Class 3 & 0.993 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\

            \item \textbf{Case 5: Naive Bayes with different covariance for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:data3g5} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3gaussian5}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of overlapping data for Naive Bayes with different covariance}
                \label{fig:data3g5}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:data3c5}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{data3c5}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for non linear seperble data for Naive Bayes with different covariance}
                \label{fig:data3c5}
            \end{figure}
            The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown.
            The confusion matrix and all the performance metrics are same for every algorithm we used. It is given in Table ~\ref{tab:d3con5}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{148}  & \cb{1}    & \cb{1}    & \cb{150}  &\cb{0.013}\\ 
                        & Class 2       & \cb{2}    & \cw{148}  & \cb{0}    & \cb{150}  &\cb{0.013}\\ 
                        & Class 3       & \cb{1}    & \cb{0}    & \cw{149}  & \cb{150}  &\cb{0.066}\\ 
                        \cline{2-7}
                        & Total         & \cb{151}  & \cb{149}  & \cb{150}  & \cb{}  &\cb{0}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.0198}    & \cb{0.0061}    & \cb{0.0067}    & \cb{0}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for overlapping data, Algorithm 5}
                    \label{tab:d3con5}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.9866 & \\
                        \cline{1-2}
                        Class 2 & 0.9866 & 98.89 \%\\
                        \cline{1-2}
                        Class 3 & 0.993 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\
            \item \textbf{Performance evaluation}
                Now with ROC and DET curves, we  can evaluate the performance. There are two possibilities, either I can see the performance of each algorithm in a particular class or i can take an algorithm and see which class achieved best performance for that model.\\
                We plot both cases;
                Table~\ref{tab:d3perfClasswise} shows ROC and DET curve for each algorithm in each class.\\
                \begin{table}[ht]
                    \begin{tabular}{ccc}
                        \subfloat[Class 1 ROC]{\includegraphics[width = 2in]{d3rocCl1}} &
                        \subfloat[Class 2 ROC]{\includegraphics[width = 2in]{d3rocCl2}} &
                        \subfloat[Class 3 ROC]{\includegraphics[width = 2in]{d3rocCl3}} \\

                        \subfloat[Class 1 DET]{\includegraphics[width = 2in]{d3detCl1}} &
                        \subfloat[Class 2 DET]{\includegraphics[width = 2in]{d3detCl2}} &
                        \subfloat[Class 3 DET]{\includegraphics[width = 2in]{d3detCl3}} 
                    \end{tabular}
                    \caption{For overlapping data, DET and ROC curves for each class}
                    \label{tab:d3perfClasswise}
                \end{table}
                Table~\ref{tab:d3perfAlgowise} shows difference in performance of binary classification of each class for a particular algorithm.\\
                \begin{table}[ht]
                    \begin{tabular}{ccc}
                        \subfloat[Algo 1 ROC]{\includegraphics[width = 2in]{d3rocAl1}} &
                        \subfloat[Algo 2 ROC]{\includegraphics[width = 2in]{d3rocAl2}} &
                        \subfloat[Algo 3 ROC]{\includegraphics[width = 2in]{d3rocAl3}} \\
                        \subfloat[Algo 4 ROC]{\includegraphics[width = 2in]{d3rocAl4}} &

                        \subfloat[Algo 5 ROC]{\includegraphics[width = 2in]{d3rocAl5}} &
                        \subfloat[Algo 1 DET]{\includegraphics[width = 2in]{d3detAl1}} \\
                        \subfloat[Algo 2 DET]{\includegraphics[width = 2in]{d3detAl2}} &
                        \subfloat[Algo 3 DET]{\includegraphics[width = 2in]{d3detAl3}} &

                        \subfloat[Algo 4 DET]{\includegraphics[width = 2in]{d3detAl4}} \\
                        \subfloat[Algo 5 DET]{\includegraphics[width = 2in]{d3detAl5}} &
                    \end{tabular}
                    \caption{For overlapping data, DET and ROC curves for each class}
                    \label{tab:d3perfAlgowise}
                \end{table}
        \end{enumerate}    
    \FloatBarrier

    \item \label{Real world data} \textbf{Real world data}\\
        As always, first lets do a scatterplot of data and draw some inferences. The scatterplot is Figure~\ref{fig:Rdata}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{realData}
            \caption{The scatterplot of real world data showing different classes}
            \label{fig:Rdata}        
        \end{figure}\\
        \FloatBarrier
        We can observe from data that:
        \begin{itemize}
            \item The data contains outliers. If we proceed without removing outliers, our mean calculation might get currupt since outliers can change mean significantly.
            \item The data has missing features, we can see it from the points lying on the left axis.
            \item The class 1 and class 2 are overlapping. It will be hard to discriminate between these two classes.
            \item For class 1 and class 2, there seems to be an upper bound for Feature 2 .
        \end{itemize}
        \textbf{Outliers}\\
        It is important to remove outliers from the training data so that the outliers do not affect the parameters of the model much, in our case mean and covariance. 
        Lets draw the Boxplot for each class in each feature. Figure~\ref{rdatabp} shows box plots.\\
        \begin{figure}[htb]
            \minipage{0.32\textwidth}
              \includegraphics[width=\linewidth]{rbp1}
              \caption{Boxplot for Class 1 data}\label{rdatabp}
            \endminipage\hfill
            \minipage{0.32\textwidth}
              \includegraphics[width=\linewidth]{rbp2}
              \caption{Boxplot for Class 2 data}
            \endminipage\hfill
            \minipage{0.32\textwidth}%
              \includegraphics[width=\linewidth]{rbp3}
              \caption{Boxplot for Class 3 data}
            \endminipage
        \end{figure}
        It is evident from the data that there are outliers.
        Unlike univariate, it is hard to classify outliers in Multivariate. In Multivariate case the distance from the mean to a point is measured using \textit{Mahalanobis Distance}. Distribution of Mahalanobis distance is given by chi squared distribution of $d$ Degrees of freedoms where $d$ is the feature dimension.\\
        The (squared) Mahalanobis Distance is given by $\mathrm{MD}_i = (\vec{x} - \bm{\mu})^T \bm{\Sigma}^{-1}(\vec{x} - \bm{\mu})$\\
        A point is said to be a outlier if the (squared) Mahalanobis distance is greater than $inv\left(\sqrt{\mathcal{X}_d ^2 (0.975)}\right)$ where $\mathcal{X}_d ^2$ is chi squared distribution of DOF $d$.

        Once they are detected, we use the attribute mean to fill in the feature which is detected as outlier.
        The figures~\ref{scatterOL} shows the scatterplot of training data after and before removing outliers.
        \begin{figure}[htb]
            \minipage{0.4\textwidth}
              \includegraphics[width=\linewidth]{rbpOL}
              \caption{Scatterplot of training data before outlier removal}\label{scatterOL}
            \endminipage\hfill
            \minipage{0.4\textwidth}
              \includegraphics[width=\linewidth]{rbpwithoutOL}
              \caption{Scatterplot of training data after outlier removal}
            \endminipage\hfill
        \end{figure}

        Now with the cleaned training data, we make 5 algorithms.
        \begin{enumerate}
            \item \textbf{Case 1: Bayes with Covariance same for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure ~\ref{fig:rdg1} shows the plot.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdgaussian1}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of Real data with bayesian model with same covariance}
                \label{fig:rdg1}
            \end{figure}\\
            The test data is plotted, the contours of each class conditional probabilities are also plotted. The plot is Figure ~\ref{fig:rdc1}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdc1}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for real data with bayesian model with same covariance}
                \label{fig:rdc1}
            \end{figure}\\
            As the gaussians are same in shape, only their mean difffer. So it gives us a linear discriminant function of which boundaries are shown. The contour lines are crossing each other because of overlapping data.\\
            The performance matrix is given in Table~\ref{tab:rdcon1}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{1614}  & \cb{225}    & \cb{0}    & \cb{1839}  &\cb{0.122}\\ 
                        & Class 2       & \cb{175}   & \cw{1691}    & \cb{0}    & \cb{1866}  &\cb{0.093}\\ 
                        & Class 3       & \cb{8}     & \cb{15}      & \cw{2268}  & \cb{2291}  &\cb{0.01}\\ 
                        \cline{2-7}
                        & Total         & \cb{1797}  & \cb{1931}  & \cb{2268}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.101}    & \cb{0.124}    & \cb{0}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Real, Case 1 Algorithm}
                    \label{tab:rdcon1}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.877 & \\
                        \cline{1-2}
                        Class 2 & 0.906 & 92.95 \%\\
                        \cline{1-2}
                        Class 3 & 0.989 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\
                        
            \item \textbf{Case 2: Bayes with Covariance different for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:rdg2} shows the plot.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdgaussian2}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of real data for bayesian model with different covariance}
                \label{fig:rdg2}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot.The plot is Figure~\ref{fig:rdc2}.\\
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdc2}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for Real data for bayesian model with different covariance}
                \label{fig:rdc2}
            \end{figure}\\
            The performance matrix is given in Table~\ref{tab:rdcon2}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{1618}  & \cb{203}   & \cb{18}    & \cb{1839}  &\cb{0.12}\\ 
                        & Class 2       & \cb{219}   & \cw{1647}  & \cb{0}     & \cb{1866}  &\cb{0.117}\\ 
                        & Class 3       & \cb{10}    & \cb{14}    & \cw{2267}  & \cb{2291}  &\cb{0.010}\\ 
                        \cline{2-7}
                        & Total         & \cb{1847}  & \cb{1864}  & \cb{2285}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.123}  & \cb{0.116}    & \cb{0.007}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Real data, Case 2 Algorithm}
                    \label{tab:rdcon2}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.87 & \\
                        \cline{1-2}
                        Class 2 & 0.886 & 92.26 \%\\
                        \cline{1-2}
                        Class 3 & 0.989 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\

            \item \textbf{Case 3: Naive Bayes with} $\bm{\Sigma}_k = \sigma_k^2\bm{I}$\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:rdg3} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdgaussian3}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of Real data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:rdg3}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric circles in the contour plot. The plot is Figure~\ref{fig:rdc3}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdc3}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for Real data for Naive Bayes with $\bm{\Sigma}_k = \sigma_k^2\bm{I}$}
                \label{fig:rdc3}
            \end{figure}
            As we can see, The shape of all the gaussians are symmetrical / circular contours.The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown.\\
            The performance matrix is given in Table~\ref{tab:rdcon3}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{1628}  & \cb{211}    & \cb{0}    & \cb{1839}  &\cb{0.114}\\ 
                        & Class 2       & \cb{291}    & \cw{1575}  & \cb{0}    & \cb{1866}  &\cb{0.1553}\\ 
                        & Class 3       & \cb{6}    & \cb{14}    & \cw{2271}  & \cb{2451}  &\cb{0.008}\\ 
                        \cline{2-7}
                        & Total         & \cb{1925}  & \cb{1900}  & \cb{2271}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.154}    & \cb{0.125}    & \cb{0}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Real data, Case 3 Algorithm}
                    \label{tab:rdcon3}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.88 & \\
                        \cline{1-2}
                        Class 2 & 0.84 & 91.24 \%\\
                        \cline{1-2}
                        Class 3 & 0.99 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\

            \item \textbf{Case 4: Naive Bayes with same covariance for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:rdg4} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdgaussian4}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of real data for Naive Bayes with same covariance}
                \label{fig:rdg4}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:rdc4}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdc4}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for real data for Naive Bayes with different covariance}
                \label{fig:rdc4}
            \end{figure}
            The covariance is same for each class, So it gives us a linear discriminant function of which boundaries are shown.
            The performance matrix is given in Table~\ref{tab:rdcon4}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{1615}  & \cb{224}    & \cb{0}    & \cb{1839}  &\cb{0.121}\\ 
                        & Class 2       & \cb{190}    & \cw{1676}  & \cb{0}    & \cb{1866}  &\cb{0.101}\\ 
                        & Class 3       & \cb{8}    & \cb{15}    & \cw{2268}  & \cb{2291}  &\cb{0.01}\\ 
                        \cline{2-7}
                        & Total         & \cb{1813}  & \cb{1915}  & \cb{2268}  & \cb{}  &\cb{}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.109}    & \cb{0.124}    & \cb{0}    & \cb{}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for real data, Case 4 Algorithm}
                    \label{tab:rdcon4}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.87 & \\
                        \cline{1-2}
                        Class 2 & 0.89 & 92.71 \%\\
                        \cline{1-2}
                        Class 3 & 0.98 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\

            \item \textbf{Case 5: Naive Bayes with different covariance for all classes}\\
            Gaussian pdf of class conditional probability is plotted and the three classes and decision boundary is seperated by different color. Figure~\ref{fig:rdg5} shows the plot.
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdgaussian5}
                \vspace{-30pt}
                \caption{Gaussian pdf of posterior probability showing decision boundary of overlapping data for Naive Bayes with different covariance}
                \label{fig:rdg5}
            \end{figure}\\
            Now the decision boundary can be seen better in a contour plot. We expect concentric ellipses in the contour plot. The plot is Figure~\ref{fig:rdc5}.\\ 
            \begin{figure}[ht]
                \centering
                \includegraphics[width=\textwidth]{rdc5}
                \vspace{-30pt}
                \caption{Contour plots and test data points showing decision boundary for real data for Naive Bayes with different covariance}
                \label{fig:rdc5}
            \end{figure}
            The covariance is different for each class, So it gives us a non linear discriminant function of which boundaries are shown.
            The confusion matrix and all the performance metrics are same for every algorithm we used. It is given in Table ~\ref{tab:rdcon5}:
            \begin{table}[ht]
                \centering
                \minipage{0.75\textwidth}
                    \begin{tabular}{c | c c c c | c | c |}
                        \multicolumn{1}{c}{} & & \multicolumn{4}{c}{Prediction} \\ \cline{2-7}
                         & & Class 1 & Class 2 & Class 3 & Total & Incl. Error \\
                        \multirow{4}{*}{\rotatebox[origin=c]{90}{Truth}}

                        & Class 1       & \cw{1641}  & \cb{193}    & \cb{5}    & \cb{150}  &\cb{0.107}\\ 
                        & Class 2       & \cb{261}    & \cw{1605}  & \cb{0}    & \cb{150}  &\cb{0.139}\\ 
                        & Class 3       & \cb{6}    & \cb{14}    & \cw{2271}  & \cb{150}  &\cb{0.008}\\ 
                        \cline{2-7}
                        & Total         & \cb{151}  & \cb{149}  & \cb{150}  & \cb{}  &\cb{0}\\ 
                        \cline{2-7}
                        & Excl. Error   & \cb{0.139}    & \cb{0.114}    & \cb{0.0022}    & \cb{0}    &\cb{}\\ 
                        \cline{2-7}

                    \end{tabular}
                    \caption{Confusion matrix for Real data, Algorithm 5}
                    \label{tab:rdcon5}
                \endminipage\hfill
                \minipage{0.25\textwidth}
                    \begin{tabular}{| l | c | c |}
                        \hline
                        & Precision & Accuracy\\
                        \hline
                        Class 1 & 0.89 & \\
                        \cline{1-2}
                        Class 2 & 0.86 & 92.01 \%\\
                        \cline{1-2}
                        Class 3 & 0.991 & \\
                        \hline
                    \end{tabular}
                    \caption{Performance metric}
                \endminipage\hfill
            \end{table}\\
            \item \textbf{Performance evaluation}
                Now with ROC and DET curves, we  can evaluate the performance. There are two possibilities, either I can see the performance of each algorithm in a particular class or i can take an algorithm and see which class achieved best performance for that model.\\
                We plot both cases;
                Table~\ref{tab:rdperfClasswise} shows ROC and DET curve for each algorithm in each class.\\
                \begin{table}[ht]
                    \begin{tabular}{ccc}
                        \subfloat[Class 1 ROC]{\includegraphics[width = 2in]{rdrocCl1}} &
                        \subfloat[Class 2 ROC]{\includegraphics[width = 2in]{rdrocCl2}} &
                        \subfloat[Class 3 ROC]{\includegraphics[width = 2in]{rdrocCl3}} \\

                        \subfloat[Class 1 DET]{\includegraphics[width = 2in]{rddetCl1}} &
                        \subfloat[Class 2 DET]{\includegraphics[width = 2in]{rddetCl2}} &
                        \subfloat[Class 3 DET]{\includegraphics[width = 2in]{rddetCl3}} 
                    \end{tabular}
                    \caption{For Real data, DET and ROC curves for each class}
                    \label{tab:rdperfClasswise}
                \end{table}
                Table~\ref{tab:rdperfAlgowise} shows difference in performance of binary classification of each class for a particular algorithm.\\
                \begin{table}[ht]
                    \begin{tabular}{ccc}
                        \subfloat[Algo 1 ROC]{\includegraphics[width = 2in]{rdrocAl1}} &
                        \subfloat[Algo 2 ROC]{\includegraphics[width = 2in]{rdrocAl2}} &
                        \subfloat[Algo 3 ROC]{\includegraphics[width = 2in]{rdrocAl3}} \\
                        \subfloat[Algo 4 ROC]{\includegraphics[width = 2in]{rdrocAl4}} &

                        \subfloat[Algo 5 ROC]{\includegraphics[width = 2in]{rdrocAl5}} &
                        \subfloat[Algo 1 DET]{\includegraphics[width = 2in]{rddetAl1}} \\
                        \subfloat[Algo 2 DET]{\includegraphics[width = 2in]{rddetAl2}} &
                        \subfloat[Algo 3 DET]{\includegraphics[width = 2in]{rddetAl3}} &

                        \subfloat[Algo 4 DET]{\includegraphics[width = 2in]{rddetAl4}} \\
                        \subfloat[Algo 5 DET]{\includegraphics[width = 2in]{rddetAl5}} &
                    \end{tabular}
                    \caption{For Real data, DET and ROC curves for each class}
                    \label{tab:rdperfAlgowise}
                \end{table}
        \end{enumerate}
\end{enumerate}

 % ============================== Content ends here
\end{questions}
% \begin{center}
% \rule{.7\textwidth}{1pt}
% \end{center}
\end{document} 